카프카 미러메이커2
==

서로 다른 두 개의 카프카 클러스터 간에 토픽을 복제하는 애플리케이션

미러링하는 애플리케이션을 만들면 되지만 토픽의 모든 것을 복제할 필요성이 있어 생겨남.

미러메이커 1
--
기본 파티션을 사용했기 때문에 복제 전 데이터와 복제 후 데이터의 파티션 정보가 달랐음.
그리고 복제하는 토픽이 달라지면 수정하기 위해 미러메이커를 재시작했어야 했음. 
양방향 토픽 복제도 안됐음.

미러메이커 2
--
1을 개선함.

단방향 토픽 복제
--
config / connect-mirror-maker.properties 를 수정하면 됨. 

설정한 후 shell 명령어로 실행.

```shell

$ bin/connect-mirror-maker.sh config/connect-mirror-maker.propeties

$ bin/kafka-topics.sh --bootstrap-server a-kafka:9092 --topic test --alter --partitions 5

```

클러스터 A의 test 토픽에 데이터를 넣음.
클러스터 B에서 A.test 토픽으로 저장됨.

클러스터 A의 파티션을 5로 변경하면 미러메이커는 기본값으로 5초마다 토픽의 설정값을 확인 후 동기화.
5초 후에 클러스터 B의 A.test 토픽의 파티션 개수도 달라짐. 

지리적 복제(Geo-Replication)
--

1. 액티브-스탠바이 클러스터 운영

서비스 애플리케이션들이 직접 통신하는 클러스터를 '액티브 클러스터'라 함.<br>
나머지 1개 클러스터를 '스탠바이 클러스터'라 부름.(재해 복구를 위한 임시 클러스터)

미러메이커2를 사용해서 액티브 클러스터의 모든 토픽을 스탠바이 클러스터에 복제하여 액티브의 예상치 못한 장애에 대응가능.

> 전통적인 방식임

재해 사유
1) 자연
2) 기술적(EMP, 데이터센터의 중단)
3) 인적(사이버공격, 사보타주)

가장 좋은 방법은 물리적인 공간 분리.<br>
-> 한국에선 액티브, 일본에선 스탠바이.

한국이 중단되더라도 지속적으로 일본에 복제하고 있으므로, 일본으로 대체 작동(failover)해서 서비스의 중단을 막을 수 있음.<br>
다만, 액티브를 스탠바이 클러스터에 데이터를 복제할 때 복제가 지연되는 현상인 복제 랙(replication lag)이 발생할 수 있음.


2. 액티브-액티브 클러스터 운영

글로벌 서비스는 최소 2개 이상의 클러스터를 둠.

한국과 영국에 클러스터를 운영해서 유저별 각 지역의 클러스터로 데이터를 보내서 사용함. 

3. 허브 앤 스포크(Hub and spoke) 클러스터 운영

소규모 카프카 클러스터를 사용하고 있을 때 각 팀의 클러스터의 데이터를 한개의 클러스터에 모아 데이터 레이크로 사용.

허브(데이터 레이크) - 스포크(허브와 연결된 선)

미러메이커2를 사용해서 각 팀에서 사용하는 클러스터에 존재하는 데이터를 수집하고, 데이터 레이크용 클러스터에서 가공, 분석해서 데이터를 찾아냄.

