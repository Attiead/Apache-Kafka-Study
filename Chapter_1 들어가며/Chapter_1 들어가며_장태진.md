카프카의 탄생
==

소스 애플리케이션과 타깃 애플리케이션의 연동에 문제가 생겨 고민이 시작됨.
소스 애플리케이션은 쇼핑몰의 클릭로그, 결제로그 등을 보낼 수 있으며,

보낼 수 있는 데이터 포맷은 거의 제한이 없음. json, tsv, avro 등

타겟 애플리케이션은 로그 적재, 로그 처리 등의 역할을 함.

ETL(Extract Transform Load) 툴을 적용해서 아키텍처를 변경해보려 했지만, 데이터 파이프라인의 복잡도를 낮추기 1차 실패

데이터 흐름의 복잡도를 개선하기 위해 개발된게 카프카.

> 카프카는 각각의 애플리케이션끼리 연결하여 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙집중화 했다.
> 카프카를 통해 웹사이트, 애플리케이션, 센서 등에서 취합한 데이터 스트림을 한 곳에서 실시간으로 관리할 수 있게 된 것.
> 카프카는 기업의 대용량 데이터를 수집하고 이를 사용자들이 실시간 스트림으로 소비할 수 있게 만들어주는 일종의 중추 신경으로 동작.

카프가가 중앙에 배치되면서 소스 애플리케이션과 타깃 애플리케이션 사이의 의존도가 최소화됨.<br>
기존 : 애플리케이션 간 직접적으로 연결되어 있었음.<br>
개선 : 중앙에서 집계 후 처리.

1:1 매칭으로 운영되던 데이터 파이프라인은 커플링(결합)으로 인해 한쪽에 영향을 줬음. 
=> 소스 애플리케이션에서 생성되는 데이터를 특정 타깃 애플리케이션으로 보낼지 고민하지 않고 카프카로 넣으면 됨.

FIFO(First In First Out) 방식의 Queue와 비슷함. 

Queue로 데이터를 보내는것이 프로듀서, 데이터를 가져가는 것이 컨슈머.
프로듀서 : 생산자, 컨슈머 : 소비자 => 데이터를 생산해서 Queue(kafka)에 집어넣으면 소비자는 차례대로 데이터를 가져간다.

- 카프카는 데이터 포맷의 제한이 없다.<br>
-> byteArray로 통신해서 직렬화, 역직렬화가 되기 때문. 

- 카프카는 최소 3대 이상의 서버(브로커)에서 분산 운영하여 프로듀서를 통해 전송받은 데이터를 파일 시스템에 안전하게 기록한다.
-> 클러스터 중 일부 서버에 장애가 발생하더라도 데이터를 지속적으로 복제하기 때문에 안전하게 운영할 수 있음<br>
-> db가 3중화 되어 있는 것과 비슷한 느낌.

- 데이터를 묶음 단위로 처리하는 배치 전송을 통해 낮은 지연과 높은 데이터 처리량도 가지게 됐음.



빅데이터 파이프라인에서 카프카의 역할
==

수십 테라 바이트가 넘어가는 방대한 양의 데이터를 기존의 데이터베이스로 관리하는 것은 불가능에 가까움. 

빅데이터를 저장하고 활용하기 위해 일단 생성되는 데이터를 모두 모으는 것에 사용되는 개념 = "데이터 레이크(data lake)"

데이터 웨어하우스와 다르게 필터링되거나 패키지화되지 않은 데이터가 저장된다.
-> 데이터 웨어하우스는 필터링, 패키지화 해서 웨어하우스 단위로 저장하는 방식을 뜻하는듯?

> 데이터 레이크는 일단 수집 가능한 모든 데이터를 모으고, 그 정보를 데이터 과학자, 분석가가 용도에 맞게 활용함.

데이터 레이크로 모으기 위해 데이터를 추출(extracting), 변경(transforming), 적재(loading)하는 과정을 묶은 데이터 파이프라인을 구축해야함.

데이터 파이프라인 : 엔드 투 엔드 방식의 데이터 수집 및 적재를 개선하고 안정성을 추구하며, 유연하면서도 확장 가능하게 자동화한 것.


카프카가 데이터 파이프라인 구축에 적합한 이유

높은 처리량
-
카프카는 데이터를 묶어서 전송함.<br>
동일한 양의 데이터를 보낼 때 네트워크 통신 횟수를 최소한으로 줄인다면 시간 내에 더 많은 데이터를 전송가능. <br>
-> 대용량의 실시간 로그데이터를 처리하는 데에 적합함.<br>
-> 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬처리 가능.<br>
-> 파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것임.

확장성
-
데이터량에 대해 가변적인 환경에서 안정적으로 확장이 가능함.<br>
데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영<br>
데이터가 많을 때는 클러스터의 브로커 개수를 자연스럽게 늘려 scale-out 할 수 있음.(scale-in도 가능)

영속성
-
다른 메시징 플랫폼과 다르게 데이터를 메모리에 저장하지 않고, 파일 시스템에 저장함. <br>
운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용함. <br>
이 것을 카프카가 활용해서 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높아짐.<br>
디스크에 저장하기 때문에 장애가 발생하여 갑자기 종료되더라도 재시작후 데이터 처리 가능!

고가용성
-
클러스터로 이루어진 카프카는 데이터의 복제를 통해 고가용성의 특징을 가짐.<br>
프로듀서로 전송받은 데이터를 여러대의 브로커에도 저장함. <br>
온프레미스 환경, 클라우스 환경에도 복제할 수 있는 브로커 옵션이 있음.

데이터 레이크 아키텍처와 카프카의 미래
==
람다 아키텍쳐와 카파 아키텍처

람다 : 레거시 데이터 수집 플랫폼을 개선하기 위해 구성한 아키텍처.
데이터가 파편화되면서 데이터 거버넌스(정책)을 지키기 어려워져서 이를 해결하기 위해 실시간 데이터 ETL 작업 영역을 정의한 <br>
speed layer를 만듬.

batch layer : 데이터를 모아서 일괄 처리<br>
serving layer : 가공된 데이터를 데이터 사용자, 서비스 애플리케이션이 사용할 수 있게 저장된 공간. 
speed layer : 서비스에서 생성되는 원천 데이터를 실시간으로 분석하는 용도로 사용됨. 

람다 아키텍쳐에서 카프카는 스피드 레이어에 위치함.<br>
-> 실시간 데이터를 짧은 지연시간으로 처리, 분석 할 수 있음. 

람다의 단점 : 
- 데이터를 분석, 처리하는데 필요한 로직이 2벌로 각가의 레이어에 따로 존재해야함.
- 배치데이터와 실시간 데이터를 융합할 때 유연하지 못함. 
- 컴파일 이후에 배치 레이어와 스피드 레이어에 각각 디버깅, 배포해야 했기 때문.


카파 : 배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣었음.<br>
파편화, 디버깅, 배포, 운영분리에 대한 이슈 제거를 위해 배치 레이어를 제거함.<br> 
-> but 스피드 레이어에서 모든 데이터를 처리하기 때문에 모든 종류의 데이터를 스트림 처리해야함.

배치데이터를 로그로 표현할 때는 각 시점의 변환 기록(변경 감지)을 기록하는 방법으로 모든 스냅샷 데이터를 저장하지 않고 표현하게 되었다.

SPOF(Single Point Of Failure), High Availability, fault tolerant 특징을 지님 ??


2020년에는 카파 아키텍처에서 서빙 레이어를 제거한 '스트리밍 데이터 레이크'를 제안함. 
-> 모든걸 스피드 레이어를 통해 관리함. <br>
=> 서빙과 스피드 레이어가 이중으로 관리되는 운영 리소스를 줄일 수 있음.<br>
=> 데이터의 중복, 비정합성 같은 문제에서 벗어날 수 있음.

정리
==
스타트업에도 유용함. 지속적으로 발전중임. 아키텍처 구성에 따라 무궁무진함. 단, 특징과 동작 방식에 대해 면밀하게 알아야함. 후...




