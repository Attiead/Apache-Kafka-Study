### 토픽과 파티션
* 데이터의 생명주기 한 가운데에 있는 토픽
* 카프카의 시작 : 토픽을 만들면서 시작
* 카프카의 끝 : 토픽을 삭제하면 데이터는 삭제되고 파이프라인은 중단
* 이 챕터에서는 토픽을 사용함에 있어 발생하는 여러가지 운영상 고려사항을 살펴봄
##### 적정 파티션 개수
* 토픽의 파티션 개수는 카프카의 성능과 관련이 있음
  * -> 파티션이 카프카의 병렬처리의 핵심이기 때문
* 토픽 최초 생성 시 파티션 개수를 정하는데 고려사항
  1. 데이터 처리량 (해당 토픽에 필요한 데이터 처리량을 측정하여 파티션 개수를 정해야 함)
    * 데이터 처리 속도 올리는 법
      * 컨슈며 처리량 늘리거나(스케일업) GC 튜닝
        * 단점 : 컨슈머 특성 상 다른 시스템들과 연동되기 때문에 일정 수준 이상 처리량을 올리는 것은 어려움
      * 파티션 개수 & 컨슈머 개수 up(스케일다운)
        * 데이터 처리량을 늘리는 가장 확실한 방법
        * 프로듀서 전송 데이터량 < 컨슈머 데이터 처리량 x 파티션 개수
          * 데이터 처리량 측정 시
            * 상용에서 운영 중인 카프카에서 더미 데이터로 테스트를 해 보는 것 (로컬의 경우 상용환경처럼 다른 시스템과 연동되는 경우를 테스트하기 어려우므로 상용에서 권장)
          * 데이터 생성량 측정 시 
            * 프로듀서가 보내는 데이터의 양을 하루, 시간, 분 단위로 쪼개서 예측
    * 파티션 개수를 무조건 늘리는 것이 좋은가?
      * 파티션 개수를 늘리면 컨슈머, 브로커의 부담이 있음
      * 따라서 지연 발생에 따른 서비스 영향도를 같이 고려하여 파티션 개수를 구하는 것이 중요함
  2. 메시지 키 사용 여부
    * 메시지 키를 사용하면, 데이터 처리 순서를 지켜야 하는 경우에 대해 고려해야 함.
    * if 파티션 개수가 달라지면 then 이미 매칭된 파티션과 메시지 키의 매칭이 깨지고, 전혀 다른 파티션에 데이터가 할당됨 
      * -> 메시지 키를 사용하는 컨슈머는 특정 메시지 키의 순서를 보장 받지 못하게 됨 (위에서 매칭이 깨지고, 메시지 키의 파티션 위치가 달라졌기 때문)
    * 따라서 메시지 키를 사용하고, 컨슈머에서 메시지 처리 순서가 보장되어야 한다면 파티션의 변화가 발생하지 않는 방향으로 운영해야 한다.
    * 그럼에도 파티션 개수가 변해야 하는 경우 -> 기존의 메시지 키의 매칭을 그대로 가져가기 위해 커스텀 파티셔너를 개발하고 적용해야 함
  3. 브로커, 컨슈머 영향도
     * 배경
       * 파티션은 각 브로커의 파일 시스템을 사용하기 때문에 파티션 개수 up -> 브로커에 접근하는 파일 개수 up
       * 그런데 운영체제에서는 프로세스당 열 수 있는 파일 최대 개수를 제한하고 있음
     * 카프카 브로커가 접근하는 파일 개수를 안정적으로 유지하기 위해서는 각 브로커당 파티션 개수를 모니터링을 해야함
     * 데이터량 증가해서 -> 파티션의 개수를 늘려야 하는 상황 -> 브로커당 파티션 개수를 확인하고 늘려야 함
     * 추가로 브로커가 관리하는 파티션 개수가 너무 많다면, 파티션 개수를 늘리기 위해 카프카 브로커 개수를 늘리는 방안도 같이 고려해야 한다.
##### 토픽 정리 정책(cleanup.policy)
* 소개
  * 토픽의 데이터는 시간 또는 용량에 따라 삭제 규칙을 적용할 수 있고, 삭제하지 않도로 설정할 수도 있다.
  * 데이터를 더는 사용하지 않을 경우에는 cleanup.policy 옵션을 사용하여 데이터를 삭제할 수 있음.
  * 삭제 정책은 두 가지
* 정책들을 미리 보기 전에...
  * 세그먼트 
  * 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위, 토픽의 데이터 삭제할 시에도 이 단위로 삭제를 진행
  * 파티션마다 별개로 생성되고, 세그먼트의 파일이름은 오프셋 중 가장 작은 값이 됨.
  * segment.bytes 옵션으로 1개의 세그먼트 크기를 설정할 수 있으며, 이 크기보다 커질 경우에는 기존에 적재하던 세그먼트 파일을 닫고 새로운 세그먼트(액티브 세그먼트)를 열어서 데이터를 저장.
* 토픽 삭제 정책(delete policy)
  * 명시적으로 토픽의 데이터를 삭제하는 것을 뜻함. 대부분을 이것으로 설정
  * 삭제 정책이 실행되는 시점의 기준
    * 시간
      * retention.ms : 토픽의 데이터를 유지하는 기간을 밀리초로 설정
      * 카프카가 일정 주기마다 세그먼트 파일의 마지막 수정 시간과 위 설정을 비교하여, 마지막 수정 시간이 위 설정을 넘어가면 삭제
    * 용량
      * retention.bytes : 토픽의 데이터 크기를 제어, 크기 넘어가면 삭제
  * 삭제된 데이터는 복구 할 수 없음.
* 토픽 압축 정책(compact policy)
  * 메시지 키별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 것
  * 이 정책은 카프카 스트림즈의 KTable과 같이 메시지 키를 기반으로 데이터를 처리할 경우 유용
    * -> 데이터의 흐름이 아닌 가장 마지막으로 업데이트된 메시지 키의 데이터가 중요할 경우 가장 최신의 데이터를 제외한 데이터를 삭제할 수 있기 때문.
    * 클린로그 : 브로커의 압축 정책에 의해 압축이 완료된 레코드(테일 영역)를 지칭, 압축이 완료되어 중복된 메시지 키가 없다.
    * 더티로그 : 압축되기 전 레코드들이 있는 영역, 중복된 메시지 키를 가진 레코드들이 있음.

##### ISR(In-Sync-Replicas)
* 소개
  * 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻함
  * 동기화된 상태에서는 리더 또는 팔로워 파티션이 위치하는 브로커에 장애가 발생하더라도 데이터를 안전하게 사용할 수 있음.
* ISR이 필요한 이유
  * 프로듀서 -> 리더 파티션(데이터 저장) : 새로운 레코드가 추가되어 오프셋이 증가
  * 리더 파티션 -> 팔로워 파티션(데이터 복제) : 리더 파티션에 오프셋이 증가하면 데이터 복제를 시작 
  * 리더 파티션은 replica.log.time.max.ms값의 주기를 가지고 팔로워 파티션이 데이터를 복제하는지 확인
  * 위 주기보다 더 긴 시간 동안 데이터를 가져가지 않는다면 해당 팔로워 파티션에 문제가 생긴 것으로 판단하여 ISR 그룹에서 제외.
  * ISR로 묶인 팔로워 파티션은 리더 파티션으로 선출될 자격을 가지게 됨 (데이터가 유실되지 않기 때문)
* unclean.leader.election.enable
  * false : ISR이 아닌 팔로워 파티션이 리더로 선출 될 수 없음
    * -> 리더 파티션이 존재하는 브로커가 다시 시작되기까지 기다리며, 기다리는 동안 토픽을 사용하는 서비스가 중단
    * -> 데이터 유실은 없음 
  * true : ISR이 아닌 팔로워 파티션도 리더로 선출 될 수 있음
    * -> 리더 파티션이 존재하는 브로커에서 장애가 발생 -> 동기화 되지 않은 팔로워 파티션이 리더로 선출 -> 서비스가 중단 되지 않음
    * -> 일부 데이터 유실 
* 그렇다면 true or false?
  * 서비스의 운영 정책에 따라 판단
  * 일부 데이터가 유실되더라도 토픽과 연동 중인 서비스의 무중단 운영이 더 중요하면 -> true
  * 데이터가 유실되면 안되는 경우 -> false
  * 이 옵션은 토픽별로 설정할 수 있다.
